{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882d368f-6805-459b-90a0-e09e6a02db7e",
   "metadata": {},
   "source": [
    "# Homework #4: Subset Selection and Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129872b-b758-4476-a65b-c80ee8a82b48",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In car sales, one of the most critical metrics is the number of days a vehicle spends on the lot. Some estimates suggest that every day a vehicle spends on the lot will cost the dealership ~$10/day in depreciation and maintenance. Multiply that by the hundreds (or thousands) of vehicles a dealership may hold in inventory and this quickly becomes one of the largest costs. A dataset provided by DriveTime, contains vehicle information as well as the number of days it spent on the lot, our task is to find any relationships that may explain the increase or decrease in days to sell.\n",
    "\n",
    "### Relevant Datasets\n",
    "\n",
    "`drive_time_sedans.csv`\n",
    "\n",
    "Source: https://github.com/Fumanguyen/drivetime-sedans-used-vehicle-market/blob/master/drive_time_sedans.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17191a8-41de-41b4-ba98-6a4650f4d44a",
   "metadata": {},
   "source": [
    "## Task 1: Import the dataset and convert the categorical variables to dummy variables.\n",
    "\n",
    "**Important Note**: The tasks below can be very computationally intensive. If you don't want to wait a long time for things to run or you don't feel your computer is powerful to complete these tasks in a reasonable time, I suggest dropping the `make.model`, `state`, and/or `makex` variables. Your grade will not be based on the inclusion or exclusion of any variables, I'm more interested in the methods but if you have the resources and are curious to explore more, feel free to use all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b14d960-5baa-44b2-b05e-4629b913074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daa4546-b8fe-459a-ab15-143ece26f0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data.set</th>\n",
       "      <th>total.cost</th>\n",
       "      <th>lot.sale.days</th>\n",
       "      <th>mileage</th>\n",
       "      <th>vehicle.age</th>\n",
       "      <th>vehicle.type_ECONOMY</th>\n",
       "      <th>vehicle.type_FAMILY.LARGE</th>\n",
       "      <th>vehicle.type_FAMILY.MEDIUM</th>\n",
       "      <th>vehicle.type_FAMILY.SMALL</th>\n",
       "      <th>vehicle.type_LUXURY</th>\n",
       "      <th>...</th>\n",
       "      <th>color.set_BLACK</th>\n",
       "      <th>color.set_BLUE</th>\n",
       "      <th>color.set_GOLD</th>\n",
       "      <th>color.set_GREEN</th>\n",
       "      <th>color.set_PURPLE</th>\n",
       "      <th>color.set_RED</th>\n",
       "      <th>color.set_SILVER</th>\n",
       "      <th>color.set_WHITE</th>\n",
       "      <th>overage_NO</th>\n",
       "      <th>overage_YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>4037</td>\n",
       "      <td>135</td>\n",
       "      <td>67341</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>4662</td>\n",
       "      <td>18</td>\n",
       "      <td>69384</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>4459</td>\n",
       "      <td>65</td>\n",
       "      <td>58239</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>4279</td>\n",
       "      <td>1</td>\n",
       "      <td>58999</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN</td>\n",
       "      <td>4472</td>\n",
       "      <td>37</td>\n",
       "      <td>47234</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  data.set  total.cost  lot.sale.days  mileage  vehicle.age  \\\n",
       "0    TRAIN        4037            135    67341            8   \n",
       "1    TRAIN        4662             18    69384            4   \n",
       "2    TRAIN        4459             65    58239            4   \n",
       "3    TRAIN        4279              1    58999            3   \n",
       "4    TRAIN        4472             37    47234            6   \n",
       "\n",
       "   vehicle.type_ECONOMY  vehicle.type_FAMILY.LARGE  \\\n",
       "0                 False                       True   \n",
       "1                 False                      False   \n",
       "2                  True                      False   \n",
       "3                  True                      False   \n",
       "4                 False                      False   \n",
       "\n",
       "   vehicle.type_FAMILY.MEDIUM  vehicle.type_FAMILY.SMALL  vehicle.type_LUXURY  \\\n",
       "0                       False                      False                False   \n",
       "1                       False                       True                False   \n",
       "2                       False                      False                False   \n",
       "3                       False                      False                False   \n",
       "4                        True                      False                False   \n",
       "\n",
       "   ...  color.set_BLACK  color.set_BLUE  color.set_GOLD  color.set_GREEN  \\\n",
       "0  ...            False           False           False            False   \n",
       "1  ...            False           False           False            False   \n",
       "2  ...            False           False           False            False   \n",
       "3  ...            False           False           False            False   \n",
       "4  ...            False            True           False            False   \n",
       "\n",
       "   color.set_PURPLE  color.set_RED  color.set_SILVER  color.set_WHITE  \\\n",
       "0             False          False              True            False   \n",
       "1             False          False              True            False   \n",
       "2             False           True             False            False   \n",
       "3             False           True             False            False   \n",
       "4             False          False             False            False   \n",
       "\n",
       "   overage_NO  overage_YES  \n",
       "0       False         True  \n",
       "1        True        False  \n",
       "2        True        False  \n",
       "3        True        False  \n",
       "4        True        False  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('drive_time_sedans.csv')\n",
    "df = df.drop(['make.model', 'state', 'makex'], axis=1)\n",
    "df = pd.get_dummies(df, columns=['vehicle.type', 'domestic.import', 'vehicle.age.group', 'color.set','overage'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05259308-58a0-49de-8fde-479db3b12d93",
   "metadata": {},
   "source": [
    "## Task 2: This dataset specifies which observations to use as train/test/validate. Split it into three dataframes based on these values.\n",
    "\n",
    "If you've already converted those to dummy variables, you may have to subset slightly different. Search \"*conditional subset pandas dataframe*\" for a starting point or reach out to me (before the soft deadline) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d3096a-a33c-44ec-93f7-cb6054934cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8753, 26)\n",
      "Test shape: (4376, 26)\n",
      "Validate shape: (4377, 26)\n",
      "total.cost                     int64\n",
      "lot.sale.days                  int64\n",
      "mileage                        int64\n",
      "vehicle.age                    int64\n",
      "vehicle.type_ECONOMY            bool\n",
      "vehicle.type_FAMILY.LARGE       bool\n",
      "vehicle.type_FAMILY.MEDIUM      bool\n",
      "vehicle.type_FAMILY.SMALL       bool\n",
      "vehicle.type_LUXURY             bool\n",
      "domestic.import_Domestic        bool\n",
      "domestic.import_Import          bool\n",
      "vehicle.age.group_FIVE          bool\n",
      "vehicle.age.group_FOUR          bool\n",
      "vehicle.age.group_ONE-THREE     bool\n",
      "vehicle.age.group_SEVEN+        bool\n",
      "vehicle.age.group_SIX           bool\n",
      "color.set_BLACK                 bool\n",
      "color.set_BLUE                  bool\n",
      "color.set_GOLD                  bool\n",
      "color.set_GREEN                 bool\n",
      "color.set_PURPLE                bool\n",
      "color.set_RED                   bool\n",
      "color.set_SILVER                bool\n",
      "color.set_WHITE                 bool\n",
      "overage_NO                      bool\n",
      "overage_YES                     bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df['data.set'] == 'TRAIN'].drop('data.set', axis=1)\n",
    "df_test = df[df['data.set'] == 'TEST'].drop('data.set', axis=1)\n",
    "df_val = df[df['data.set'] == 'VALIDATE'].drop('data.set', axis=1)\n",
    "\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Test shape:', df_test.shape)\n",
    "print('Validate shape:', df_val.shape)\n",
    "\n",
    "print(df_val.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c3ad5-2fa5-446f-8e49-47fb01989b11",
   "metadata": {},
   "source": [
    "## Task 3: Normalize `total.cost`, `mileage`, and `vehicle.age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "811306b8-5f5e-4b26-9213-96fb0ad6912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = ['total.cost', 'mileage', 'vehicle.age']\n",
    "scaler = StandardScaler()\n",
    "df_train[cols_to_normalize] = scaler.fit_transform(df_train[cols_to_normalize])\n",
    "df_test[cols_to_normalize] = scaler.transform(df_test[cols_to_normalize])\n",
    "df_val[cols_to_normalize] = scaler.transform(df_val[cols_to_normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c075e4-279f-430e-bc04-ba33db886c77",
   "metadata": {},
   "source": [
    "## Task 4: Use the code from the applied lecture to perform forward stepwise selection, with the single validation set from before (as opposed to cross-validation). Return not only the AIC, BIC, and Adjusted $R^2$, as was shown in the lecture, but also the MSE on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544ff121-3f41-4f8b-b58a-5ac6db9ad5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(X, y, predictor_variables, response_variable):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    \n",
    "    model = sm.OLS(y,X[list(predictor_variables)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(predictor_variables)]) - y[response_variable]) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def forward(X, y, predictors, response_variable):\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    results = []\n",
    "\n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(X, y, predictors + [p], response_variable))\n",
    "\n",
    "    models = pd.DataFrame(results)\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba6618b0-8cca-412a-b315-6824c92130b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8753, 25)\n",
      "(8753, 1)\n",
      "(4377, 25)\n",
      "(4377, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_val\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):    \n\u001b[0;32m---> 16\u001b[0m     models_fwd\u001b[38;5;241m.\u001b[39mloc[i] \u001b[38;5;241m=\u001b[39m forward(X_train, y_train, predictors, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlot.sale.days\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     predictors \u001b[38;5;241m=\u001b[39m models_fwd\u001b[38;5;241m.\u001b[39mloc[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mexog_names\n\u001b[1;32m     18\u001b[0m     models_fwd\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIC\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m models_fwd\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39maic\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(X, y, predictors, response_variable)\u001b[0m\n\u001b[1;32m     11\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m remaining_predictors:\n\u001b[0;32m---> 14\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(processSubset(X, y, predictors \u001b[38;5;241m+\u001b[39m [p], response_variable))\n\u001b[1;32m     16\u001b[0m models \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[1;32m     17\u001b[0m best_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mloc[models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRSS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39margmin()]\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mprocessSubset\u001b[0;34m(X, y, predictor_variables, response_variable)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessSubset\u001b[39m(X, y, predictor_variables, response_variable):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Fit model on feature_set and calculate RSS\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(y,X[predictor_variables])\n\u001b[1;32m      5\u001b[0m     regr \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m      6\u001b[0m     RSS \u001b[38;5;241m=\u001b[39m ((regr\u001b[38;5;241m.\u001b[39mpredict(X[\u001b[38;5;28mlist\u001b[39m(predictor_variables)]) \u001b[38;5;241m-\u001b[39m y[response_variable]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:922\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    921\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28msuper\u001b[39m(OLS, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[1;32m    923\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:748\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28msuper\u001b[39m(WLS, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[1;32m    749\u001b[0m                           weights\u001b[38;5;241m=\u001b[39mweights, hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    750\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    751\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:202\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28msuper\u001b[39m(RegressionModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[1;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/base/data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[1;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/base/data.py:84\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/statsmodels/base/data.py:509\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    507\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(PandasData, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop('lot.sale.days', axis=1)\n",
    "y_train = df_train[['lot.sale.days']]\n",
    "X_val = df_val.drop('lot.sale.days', axis=1)\n",
    "y_val = df_val[['lot.sale.days']]\n",
    "\n",
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\", \"AIC\", \"BIC\", \"AdjR2\", \"MSE\"])\n",
    "\n",
    "predictors = []\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "\n",
    "for i in range(1,len(X_train.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(X_train, y_train, predictors, 'lot.sale.days')\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "    models_fwd.loc[i, 'AIC'] = models_fwd.loc[i, 'model'].aic\n",
    "    models_fwd.loc[i, 'BIC'] = models_fwd.loc[i, 'model'].bic\n",
    "    models_fwd.loc[i, 'AdjR2'] = models_fwd.loc[i, 'model'].rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ad887-36ec-48af-a118-d81c86d280b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(X_train.columns)+1):   \n",
    "    models_fwd.loc[i] = forward(X_train, y_train, predictors, 'lot.sale.days')\n",
    "    predictors = models_fwd.loc[i]['model'].model.exog_names\n",
    "    print(y_val.shape)\n",
    "    print(models_fwd.loc[i, 'model'].predict(X_val).shape)\n",
    "    models_fwd.loc[i, 'AIC'] = models_fwd.loc[i, 'model'].aic\n",
    "    models_fwd.loc[i, 'BIC'] = models_fwd.loc[i, 'model'].bic\n",
    "    models_fwd.loc[i, 'AdjR2'] = models_fwd.loc[i, 'model'].rsquared_adj\n",
    "    # print(mean_squared_error(y_val, models_fwd.loc[i, 'model'].predict(X_val)))\n",
    "    print(mean_squared_error(y_val, models_fwd.loc[i, 'model'].predict(X_val)).shape)\n",
    "    print(models_fwd.loc[i,'MSE'].shape)\n",
    "    models_fwd.loc[i, 'MSE'] = mean_squared_error(y_val, models_fwd.loc[i, 'model'].predict(X_val))\n",
    "\n",
    "print(models_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3873f2-b33e-4ad7-8939-d87b8b0c1193",
   "metadata": {},
   "source": [
    "## Task 5: Using the code from the shrinkage methods lecture, find the optimal $\\alpha$ and $\\lambda$ for an Elastic Net regression using Cross-Validation.\n",
    "\n",
    "Note: Remember that $\\lambda$ is the argument `alpha` in scikit-learn and $\\alpha$ is the `l1_ratio` argument. Sorry that nobody can settle on terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8a1bf-3ca0-46ca-ba56-ee970f21feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "l1_ratio = 0.5\n",
    "\n",
    "en = ElasticNet(max_iter=10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    en.set_params(alpha=a, l1_ratio=l1_ratio)\n",
    "    en.fit(scale(X_train), y_train)\n",
    "    coefs.append(en.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Elastic Net coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2a3b7-ad3f-40fc-9144-c99c34866733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = np.random.choice(a = 10, size = X_train.shape[0])\n",
    "alphas = np.linspace(0.01,5,10)\n",
    "l1_ratio = np.linspace(0.01,1,10)\n",
    "\n",
    "mse_list = []\n",
    "alpha_list = []\n",
    "l1_list = []\n",
    "for alpha in alphas:\n",
    "    for l1 in l1_ratio:\n",
    "        cv_list = []\n",
    "        for i in range(10):\n",
    "            val_folds_X = X_train.iloc[np.where(folds == i)]\n",
    "            train_folds_X = X_train.iloc[np.where(folds != i)]\n",
    "\n",
    "            val_folds_y = y_train.iloc[np.where(folds == i)]\n",
    "            train_folds_y = y_train.iloc[np.where(folds != i)]\n",
    "\n",
    "            encv = ElasticNet(alpha=alpha, l1_ratio=l1, max_iter=10000)\n",
    "            encv.fit(scaler.transform(train_folds_X), train_folds_y)\n",
    "            pred = encv.predict(scaler.transform(val_folds_X))\n",
    "            cv_list.append(mean_squared_error(val_folds_y, pred))\n",
    "        mse_list.append(np.mean(cv_list))\n",
    "        alpha_list.append(alpha)\n",
    "        l1_list.append(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718f831-dde0-4492-b2ad-5ea866a1d90f",
   "metadata": {},
   "source": [
    "## Question: Given all of the results you've found, which model would you choose and why? Hint: There is no right answer but you will need to justify any answer you give."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
