{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f4f07b-dfec-49b8-8154-2d2dfc02a28b",
   "metadata": {},
   "source": [
    "# Unit Test 3\n",
    "\n",
    "Topics Covered:\n",
    "* Generalized Linear Models\n",
    "* K-Nearest Neighbors\n",
    "* CART\n",
    "* Random Forests\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "\n",
    "## Background\n",
    "Story telling is a key component of interpersonal communication and the study of narrative ability in children can provide critical insights into their language development. Narrative sample analysis is a process in which an individual produces a narrative and then a Speech-Language Pathologist (or similar practitioner) analyzes the quality. One tool for measuring this quality is the Monitoring Indicators of Scholarly Language (MISL). It provides an objective measure of the macrostructure story elements (e.g. Characters, Setting, Initiating Event) as well as the microstructure or grammatical elements. \n",
    "\n",
    "The process of scoring the macrostructure can be very time consuming though, which leads to less effective ongoing monitoring. This dataset provides the first publicly accessible data for attempting to automate scoring of the macrostructure via Machine Learning.\n",
    "\n",
    "## Dataset:\n",
    "`AutomatedNarrativeAnalysisMISLData.csv`\n",
    "\n",
    "## Task\n",
    "\n",
    "Your goal is to predict the Initiating Event (`IE`) label. The `IE` is scored as either 0, 1, 2, or 3 but for our purposes it is acceptable to predict this as either a continuous or categorical output. Note that if you predict it as continuous, it is necessary to constrain the prediction in some way, therefore, categorical may be easier.\n",
    "\n",
    "For predictor variables, you have two choices: either the raw text or the text features (or both, technically). The text features are every column **except** `Char`, `Sett`, `IE`, `Plan`, `Act`, and `Con`. Those 6 variables are the output scores but again we'll just be focusing on `IE` for now. Also, exclude the `ID` column.\n",
    "\n",
    "Using cross-validation, explore the many different classification algorithms we discussed to find the model with the highest performance (I'll leave it to you to define performance).\n",
    "\n",
    "**Bonus**: The column `vecOfNarratives` contains the raw text. If you would like, feel free to use Tf-Idf method for creating columns out of raw text that we discussed in the SVM lecture. It's in the notebook titled `BBC_Text_preprocessing.ipynb`.\n",
    "\n",
    "*This is still very much an open task so any major improvements would likely be publication worthy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca466ac9-c2d1-4e81-9495-57b28f08070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd907b01-ae23-4552-b904-63d1fb3cacb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "df = pd.read_csv('AutomatedNarrativeAnalysisMISLData.csv')\n",
    "\n",
    "# Clean data\n",
    "df = df.drop(columns=['ID', 'vecOfNarratives', 'Char', 'Sett', 'Plan', 'Act', 'Con']) # Drop columns\n",
    "df = df.dropna() # Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fab6629-82e9-488f-9262-f6d4f2558d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into features and target variable\n",
    "X = df.drop(columns=['IE'])\n",
    "y = df['IE']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58c570f-4be3-488a-899b-ecba4a236120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Logistic Regression:\n",
      "\tCross-validated accuracy scores: [0.56716418 0.5        0.48484848 0.40909091 0.5       ]\n",
      "\tMean accuracy: 0.4922207146087743\n",
      "\tStandard deviation: 0.05040331689500523\n",
      "\n",
      "Evaluating K-Nearest Neighbors:\n",
      "\tCross-validated accuracy scores: [0.52238806 0.46969697 0.48484848 0.5        0.42424242]\n",
      "\tMean accuracy: 0.48023518769787427\n",
      "\tStandard deviation: 0.03296979991478287\n",
      "\n",
      "Evaluating Decision Tree:\n",
      "\tCross-validated accuracy scores: [0.41791045 0.42424242 0.28787879 0.54545455 0.43939394]\n",
      "\tMean accuracy: 0.42297602894617814\n",
      "\tStandard deviation: 0.08191344603100235\n",
      "\n",
      "Evaluating Random Forest:\n",
      "\tCross-validated accuracy scores: [0.6119403  0.5        0.56060606 0.48484848 0.59090909]\n",
      "\tMean accuracy: 0.5496607869742197\n",
      "\tStandard deviation: 0.049732861206520367\n",
      "\n",
      "Evaluating Support Vector Machine:\n",
      "\tCross-validated accuracy scores: [0.64179104 0.45454545 0.46969697 0.51515152 0.57575758]\n",
      "\tMean accuracy: 0.5313885119855268\n",
      "\tStandard deviation: 0.06947182882212444\n",
      "\n",
      "Performance of Logistic Regression on the test set:\n",
      "Accuracy: 0.4457831325301205\n",
      "F1 Score: 0.42730567502827266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.41      0.47        17\n",
      "           1       0.19      0.17      0.18        23\n",
      "           2       0.54      0.69      0.61        36\n",
      "           3       0.33      0.14      0.20         7\n",
      "\n",
      "    accuracy                           0.45        83\n",
      "   macro avg       0.40      0.36      0.36        83\n",
      "weighted avg       0.43      0.45      0.43        83\n",
      "\n",
      "\n",
      "\n",
      "Performance of K-Nearest Neighbors on the test set:\n",
      "Accuracy: 0.46987951807228917\n",
      "F1 Score: 0.4500887064742486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.41      0.56        17\n",
      "           1       0.33      0.17      0.23        23\n",
      "           2       0.45      0.69      0.55        36\n",
      "           3       0.38      0.43      0.40         7\n",
      "\n",
      "    accuracy                           0.47        83\n",
      "   macro avg       0.51      0.43      0.43        83\n",
      "weighted avg       0.50      0.47      0.45        83\n",
      "\n",
      "\n",
      "\n",
      "Performance of Decision Tree on the test set:\n",
      "Accuracy: 0.4819277108433735\n",
      "F1 Score: 0.4827117995792694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.65      0.61        17\n",
      "           1       0.47      0.39      0.43        23\n",
      "           2       0.50      0.47      0.49        36\n",
      "           3       0.27      0.43      0.33         7\n",
      "\n",
      "    accuracy                           0.48        83\n",
      "   macro avg       0.46      0.48      0.46        83\n",
      "weighted avg       0.49      0.48      0.48        83\n",
      "\n",
      "\n",
      "\n",
      "Performance of Random Forest on the test set:\n",
      "Accuracy: 0.46987951807228917\n",
      "F1 Score: 0.45901622742018783\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.53      0.60        17\n",
      "           1       0.20      0.17      0.19        23\n",
      "           2       0.51      0.67      0.58        36\n",
      "           3       0.67      0.29      0.40         7\n",
      "\n",
      "    accuracy                           0.47        83\n",
      "   macro avg       0.52      0.41      0.44        83\n",
      "weighted avg       0.47      0.47      0.46        83\n",
      "\n",
      "\n",
      "\n",
      "Performance of Support Vector Machine on the test set:\n",
      "Accuracy: 0.5301204819277109\n",
      "F1 Score: 0.4883527435660328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.59      0.69        17\n",
      "           1       0.23      0.13      0.17        23\n",
      "           2       0.53      0.83      0.65        36\n",
      "           3       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.53        83\n",
      "   macro avg       0.65      0.42      0.44        83\n",
      "weighted avg       0.55      0.53      0.49        83\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate_model(model, X_train, y_train):\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"\\tCross-validated accuracy scores: {cv_scores}\")\n",
    "    print(f\"\\tMean accuracy: {np.mean(cv_scores)}\")\n",
    "    print(f\"\\tStandard deviation: {np.std(cv_scores)}\\n\")\n",
    "    \n",
    "    # Train the model on training set and return it\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# List of models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Support Vector Machine\": SVC()}\n",
    "\n",
    "# Evaluate each model on the training set\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}:\")\n",
    "    trained_model = evaluate_model(model, X_train, y_train)\n",
    "    models[name] = trained_model\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Performance of {name} on the test set:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0632c23-924a-46c3-9aa4-67c7d76400c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best SVM cross-validated accuracy: 0.5313885119855268\n",
      "Performance of the best SVM on the test set:\n",
      "Accuracy: 0.5301204819277109\n",
      "F1 Score: 0.4883527435660328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.59      0.69        17\n",
      "           1       0.23      0.13      0.17        23\n",
      "           2       0.53      0.83      0.65        36\n",
      "           3       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.53        83\n",
      "   macro avg       0.65      0.42      0.44        83\n",
      "weighted avg       0.55      0.53      0.49        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for SVM\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# GridSearchCV object for SVM\n",
    "svm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search for SVM\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "best_svm = svm_grid_search.best_estimator_\n",
    "print(f\"Best SVM parameters: {svm_grid_search.best_params_}\")\n",
    "print(f\"Best SVM cross-validated accuracy: {svm_grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate the best SVM model on the test set\n",
    "print(\"Performance of the best SVM on the test set:\")\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_svm, average='weighted')}\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
