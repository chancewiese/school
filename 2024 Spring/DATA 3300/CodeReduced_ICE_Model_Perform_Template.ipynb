{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise - Model Performance**\n",
        "# DATA 3300"
      ],
      "metadata": {
        "id": "PmYJgDFakO_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name:"
      ],
      "metadata": {
        "id": "xSsHsEpOkXnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1\n",
        "\n",
        "**Using the (full) `voters.csv` dataset, conduct a 5-fold cross-validated logistic regression analysis in Python. Assume the data set has already been checked for collinear independent variables, and found none.**\n",
        "\n",
        "*   Import the required libraries and packages\n",
        "*   Import and view the dataset\n",
        "*   Assign IVs to object called 'x', take any preprocessing steps\n",
        "*   Assign DV to object called 'y', take any preprocessing steps\n",
        "*   Perform 90-10 train-test split\n",
        "*   Implement 5-fold cross-validated logistic regression\n",
        "\n"
      ],
      "metadata": {
        "id": "3qFDX7RDkaxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "vLmMdSAjnWCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXHnc4OQvjJK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('')                                                                                      # reads in the dataset\n",
        "# produce a heading of the dataset"
      ],
      "metadata": {
        "id": "_lgkO0AmlPEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop(['Primary Key', 'DV'], axis=1)                                                                # remove non-IVs when creating x-object\n",
        "x = pd.get_dummies(data = ?, drop_first = ?)                                                              # dummy codes categorical IVS\n",
        "\n",
        "y = df['DV']                                                                                              # create y object\n",
        "y = pd.get_dummies(data = ?, drop_first = ?)                                                              # dummy codes the y object"
      ],
      "metadata": {
        "id": "61SxWwY0lmRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size = ?, random_state = 100)                                                              # creates a 90-10 train-test split\n",
        "\n",
        "print(x_train.shape)                                                                                      # examines the shape of the training and test set objects\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "NjuOsqs6kqfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = KFold(n_splits = ?, random_state = 1, shuffle = True)                                                # produces a 5-fold cross-validation\n",
        "\n",
        "# set model to Logistic regression"
      ],
      "metadata": {
        "id": "FKE40K46l5XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scoring = {'acc': 'accuracy',                                                                            # creates a scoring dictionary\n",
        "           'f1' : 'f1',\n",
        "           'precision' : 'precision',\n",
        "           'recall' : 'recall',\n",
        "           'roc_auc' : 'roc_auc',\n",
        "           'r2' : 'r2'}"
      ],
      "metadata": {
        "id": "Yco24PTtnZQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_validate(?, ?, ?, scoring = scoring, cv = cv, return_train_score=False)                   # train model using 5-fold cross validation on training set\n",
        "# display the scores"
      ],
      "metadata": {
        "id": "JceZclYyoNNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2\n",
        "Display the average cross-validated accuracy, f1-score, precision, recall, ROC-AUC, and $R^2$"
      ],
      "metadata": {
        "id": "TXtdGmE9vUgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pd.DataFrame(scores, columns = scores.keys())                                                  # creates a dataframe of each score type and it's score\n",
        "# display the average scores, why?"
      ],
      "metadata": {
        "id": "_4o1EpW_rSDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2A\n",
        "\n",
        "**List the $R^2$ value and interpret what it means.**"
      ],
      "metadata": {
        "id": "kS1hc2havCpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DRLVV7VGvsR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2B\n",
        "\n",
        "**List the ROC-AUC value and interpret what it means.**"
      ],
      "metadata": {
        "id": "09R4Go0ZxDUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tABrjhMpxQXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3\n",
        "\n",
        "**Generate a confusion matrix of the predicted outcome vs the actual outcome (`VIntent=Kodos`).**"
      ],
      "metadata": {
        "id": "tRG1Sx7OxXkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cross_val_predict(model, x_train, y_train, cv = cv)                                            # makes cross-validated predictions onto the training set\n",
        "conf = confusion_matrix(?, ?)                                                                           # generate a confusion matrix of predicted against actual\n",
        "\n",
        "sns.heatmap(conf, annot=True, fmt='g')\n",
        "sns.set(rc={'figure.figsize':(12,10)})\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCMxGpe2xLNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3A\n",
        "\n",
        "**If `Vintent=Kodos` is the positive class, identify what TP, FP, TN, and FN mean in the context of this dataset, and provide the number of cases for each.**"
      ],
      "metadata": {
        "id": "9KKaMvkX561G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TP** =\n",
        "* **FP** =\n",
        "* **TN** =\n",
        "* **FN** ="
      ],
      "metadata": {
        "id": "OKJzx-ke6Ibt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3B\n",
        "\n",
        "**Identify the costs of both types of model errors (FP and FN) for this specific dataset. Are these costs about the same or does one error cost more than the other?**"
      ],
      "metadata": {
        "id": "uR-9KFng6qhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ss8JSAWQ6wKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3C\n",
        "\n",
        "**Is the distribution of the two outcome classes of the DV about even (within 60:40) or uneven (highly skewed)? State how you know this.**"
      ],
      "metadata": {
        "id": "_VanL4oJ6zjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display value_counts of VIntent"
      ],
      "metadata": {
        "id": "hxuhREkt66XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "us5lGZm0698K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D\n",
        "\n",
        "**What is the overall (cross-validated) model accuracy? Given this data set, is this a good model performance metric to use? Why or why not?**"
      ],
      "metadata": {
        "id": "xfD2HlnG7QgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7WDR9Sdb7Wjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3E\n",
        "\n",
        "**Calculate the baseline accuracy using naive/apriori prediction. VIntent=Kodos is the positive class outcome. Is our model performing well? Why or why not?**"
      ],
      "metadata": {
        "id": "c1mG_fWQ7lYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('baseline accuracy =', ?/?)"
      ],
      "metadata": {
        "id": "6USkPqSC7uzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pra452hC-HyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3F\n",
        "\n",
        "**What is the precision of the positive class and what does this mean? When should class precision be used to assess model peformance?**"
      ],
      "metadata": {
        "id": "h9FaM-up-hBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Aq9z6pQ-2L_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3G\n",
        "\n",
        "**What is the recall of the positive class and what does this mean? When should recall be used to assess model performance?**"
      ],
      "metadata": {
        "id": "J3_28gPt_NWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t4J_1cR7_YTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3H\n",
        "\n",
        "**What is the $f_1$ score and what does it mean? Also describe when this metric should be used.**"
      ],
      "metadata": {
        "id": "r_qkjqpIDAjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fsb8PzYgDJdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4\n",
        "\n",
        "**Develop and run a second Logistic Regression model with 5-fold cross-validation that excludes age, homeowner, income category, marital status and religion as IVs.**\n",
        "\n",
        "**Then compute the accuracy, precision, recall, f1, and auc-score.**"
      ],
      "metadata": {
        "id": "HLPA_PhqDdYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pull up columns in x_train"
      ],
      "metadata": {
        "id": "ddaVVGznDzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_2 = x_train[['', '', '', '']]                                                         # include only the remaining variables in x_train_2"
      ],
      "metadata": {
        "id": "S01rSMT1DrZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores2 = cross_validate(?, ?, ?, scoring = scoring, cv = cv, return_train_score=False)       # run cross-validated regression on updated training data"
      ],
      "metadata": {
        "id": "jKgPWiPND9av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores2 = pd.DataFrame(scores2, columns = scores2.keys())                                     # creates a dataframe of score names and scores\n",
        "np.abs(?.mean())                                                                              # take the absolute mean"
      ],
      "metadata": {
        "id": "DTeSuaEIEFbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cross_val_predict(model, x_train_2, y_train, cv = cv)                                # makes cross-validated predictions on training data"
      ],
      "metadata": {
        "id": "uKY3HGiGHjLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5\n",
        "\n",
        "**Compare Model 1 and Model 2 on their performance metrics by displaying both model's cross-validated metrics in a table below.**"
      ],
      "metadata": {
        "id": "DeELGzeNFNrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = np.abs(scores.mean())                                                                                 # stores your average CV scores to model_1\n",
        "model_2 = np.abs(scores2.mean())                                                                                # stores your average CV scores to model_2\n",
        "models = model_1, model_2                                                                                       # creates a models object of model_1 and model_2 scores"
      ],
      "metadata": {
        "id": "QZ7R33OiK2UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_compare = pd.DataFrame(data = ?,\n",
        "                        index = [\"?\", \"?\"],                                                                    # produce a dataframe to display the scores of model_1 and model_2\n",
        "                        columns = [\"?\",\n",
        "                                   \"?\",\n",
        "                                   \"?\",\n",
        "                                   \"test_f1\",\n",
        "                                   \"test_precision\",\n",
        "                                   \"test_recall\",\n",
        "                                   \"test_roc_auc\",\n",
        "                                   \"test_r2\"])\n",
        "model_compare"
      ],
      "metadata": {
        "id": "M1oYgptVo2c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5A\n",
        "\n",
        "**Which model performed best overall? How do you know?**"
      ],
      "metadata": {
        "id": "F2u7LIh-p2E2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nz99H4Fup6z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5B\n",
        "\n",
        "**Fit the best performing model to the test set. How does the model perform on the test-set? Is there any evidence of over-fitting?**"
      ],
      "metadata": {
        "id": "BzBRPk7lqUt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "?.fit(x_train, y_train)                                                                     # fit best performing model to training data\n",
        "\n",
        "predictions = ?.predict(x_test)                                                             # make predictions onto x_test"
      ],
      "metadata": {
        "id": "Q80xCh6mqXFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(?, ?))                                                          # print classification report on test set performance"
      ],
      "metadata": {
        "id": "G7QRCDK7q11i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oDsA_s19raOM"
      }
    }
  ]
}